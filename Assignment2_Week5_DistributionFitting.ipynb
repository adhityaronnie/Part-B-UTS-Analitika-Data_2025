{"cells":[{"cell_type":"markdown","id":"03b0e976","metadata":{"id":"03b0e976"},"source":["\n","# Assignment 2 – Fit & Interpret Probability Distributions for Claim Severity\n","\n","**Course:** Data Analytics for Actuarial Science  \n","**Week:** 5  \n","**Dataset:** `claim_severity.csv` (column: `claim_amount`)  \n","**Deliverables:** Notebook + brief PDF summary + figures\n","\n","**Objectives**\n","- Diagnose heavy-tailed behavior in claim severities\n","- Fit Lognormal, Gamma, Weibull, Pareto via MLE\n","- Compare models (AIC/BIC, QQ, KS/AD where applicable)\n","- Compute VaR/TVaR and interpret actuarially\n"]},{"cell_type":"markdown","id":"c66b1a19","metadata":{"id":"c66b1a19"},"source":["## 0. Setup & Load"]},{"cell_type":"code","execution_count":null,"id":"7586ff25","metadata":{"id":"7586ff25"},"outputs":[],"source":["\n","import numpy as np, pandas as pd\n","import matplotlib.pyplot as plt\n","import scipy.stats as st\n","\n","\n","# Load dataset\n","df = pd.read_csv('claim_severity.csv')  # adjust path if needed\n","x = df['claim_amount'].astype(float).values\n","n = x.size\n","print('n =', n)\n","df.head()"]},{"cell_type":"markdown","id":"f34574e1","metadata":{"id":"f34574e1"},"source":["## Part A — Exploratory Data Analysis (EDA)"]},{"cell_type":"code","execution_count":null,"id":"ed444045","metadata":{"id":"ed444045"},"outputs":[],"source":["\n","# A1. Summary stats\n","s = pd.Series(x)\n","# Make summary stat contained n, mean, median, std, cv, skew and kurtosis\n","summary = {\n","# please fill with necessary code\n","}\n","summary"]},{"cell_type":"markdown","source":["> **Discuss the summary stats!**"],"metadata":{"id":"TVjLBfQFCXXB"},"id":"TVjLBfQFCXXB"},{"cell_type":"code","execution_count":null,"id":"1e0fc879","metadata":{"id":"1e0fc879"},"outputs":[],"source":["\n","# A2. Plots\n","# Histogram (density) on log-x\n","# please fill with necessary code\n","\n","\n","# Empirical CDF\n","# please fill with necessary code\n","\n","# Mean Excess Plot\n","# please fill with necessary code\n"]},{"cell_type":"markdown","id":"fc9458ea","metadata":{"id":"fc9458ea"},"source":["> **Write 2–3 sentences interpreting tail heaviness and log-scale behavior.**"]},{"cell_type":"markdown","id":"b3846bf9","metadata":{"id":"b3846bf9"},"source":["## Part B — Parametric Fits via MLE"]},{"cell_type":"code","execution_count":null,"id":"cb276d26","metadata":{"id":"cb276d26"},"outputs":[],"source":["\n","# B1. Fit Lognormal, Gamma, Weibull, Pareto\n","ln_shape, ln_loc, ln_scale = st.lognorm.fit(x, floc=0)        # Lognormal\n","                                                              # Gamma (please fill with necessary code)\n","                                                              # Weibull (please fill with necessary code)\n","                                                              # Pareto (please fill with necessary code)\n","\n","#extract the estimated parameters\n","#please fill with necessary code\n","params = {\n","    \"Lognormal\": {},\n","    \"Gamma\": {},\n","    \"Weibull\": {},\n","    \"Pareto\": {}\n","}\n","params"]},{"cell_type":"markdown","source":["> **Discuss parametric fitting process (Theories and methods) for each distribution!**"],"metadata":{"id":"pOZjgcDBYZXx"},"id":"pOZjgcDBYZXx"},{"cell_type":"code","execution_count":null,"id":"adfc7804","metadata":{"id":"adfc7804"},"outputs":[],"source":["\n","# B3. Overlay fitted densities (log-x)\n","#please fill with necessary code\n","xx = np.linspace(x.min(), x.max(), 500)\n","plt.figure()\n","plt.hist()\n","plt.plot( , label='Lognormal')\n","plt.plot( , label='Gamma')\n","plt.plot( , label='Weibull')\n","plt.plot( , label='Pareto')\n","plt.xscale('log')\n","plt.title('Fitted PDFs (log-x)')\n","plt.xlabel('claim_amount'); plt.ylabel('density')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","source":["> **Discuss all figures and recommend the best visual fit distribution!**"],"metadata":{"id":"j1plyrZBY-Vo"},"id":"j1plyrZBY-Vo"},{"cell_type":"markdown","id":"3d92673c","metadata":{"id":"3d92673c"},"source":["## Part C — Model Adequacy & Selection"]},{"cell_type":"code","execution_count":null,"id":"5ecf1d27","metadata":{"id":"5ecf1d27"},"outputs":[],"source":["\n","def aic(ll, k): return 2*k - 2*ll\n","def bic(ll, k, n): return np.log(n)*k - 2*ll\n","\n","ll_ln = np.sum())#log-likelihood for Lognormal dist, please fill with necessary code\n","ll_ga = np.sum() #log-likelihood for Gamma, dist please fill with necessary code\n","ll_wb = np.sum() #log-likelihood for Weibull dist, please fill with necessary code\n","ll_pa = np.sum(st.pareto.logpdf(x, pa_b, pa_loc, pa_scale)) #log-likelihood for Pareto  dist\n","\n","k_ln = k_ga = k_wb = k_pa = 2\n","\n","# display diagnostic AIC/BIC results\n","info = pd.DataFrame({\n","    \"model\": [\"Lognormal\",\"Gamma\",\"Weibull\",\"Pareto\"],\n","    \"loglik\": [ll_ln, ll_ga, ll_wb, ll_pa],\n","    \"AIC\": [aic(ll_ln,k_ln), aic(ll_ga,k_ga), aic(ll_wb,k_wb), aic(ll_pa,k_pa)],\n","    \"BIC\": [bic(ll_ln,k_ln,n), bic(ll_ga,k_ga,n), bic(ll_wb,k_wb,n), bic(ll_pa,k_pa,n)]\n","}).sort_values([\"AIC\",\"BIC\"])\n","info\n","# display diagnostic QQ/KS/AD (QQ-plot, Kolmogorov-Smirnov and Andersen-Darling)\n","# please fill with necessary code\n"]},{"cell_type":"markdown","id":"20e3bb89","metadata":{"id":"20e3bb89"},"source":["> **Discuss diagnostics AIC/BIC, compare with QQ/KS/AD and select a preferred model with justification.**"]},{"cell_type":"markdown","id":"032c4f8c","metadata":{"id":"032c4f8c"},"source":["## Part D — Tail Risk & Interpretation"]},{"cell_type":"code","execution_count":null,"id":"99e04ec8","metadata":{"id":"99e04ec8"},"outputs":[],"source":["\n","from scipy.stats import norm\n","\n","# Choose your preferred model (edit this string after selection)\n","preferred = \"Lognormal\"\n","\n","def var_lognorm(p, mu, sigma):\n","    return np.exp(mu + sigma*norm.ppf(p))\n","\n","def tvar_lognorm(p, mu, sigma):\n","    num = 1 - norm.cdf(norm.ppf(p) - sigma)\n","    return np.exp(mu + 0.5*sigma**2) * (num / (1-p))\n","\n","if preferred == \"Lognormal\":\n","    mu_hat = float(np.log(ln_scale))\n","    sigma_hat = float(ln_shape)\n","    for p in [0.95, 0.99]:\n","        print(p, \"VaR\", var_lognorm(p, mu_hat, sigma_hat), \"TVaR\", tvar_lognorm(p, mu_hat, sigma_hat))\n","else:\n","    print(\"Implement VaR/TVaR for your chosen model.\")"]},{"cell_type":"markdown","id":"623a13aa","metadata":{"id":"623a13aa"},"source":["> **Interpret VaR/TVaR for pricing and capital (1–2 paragraphs).**"]},{"cell_type":"markdown","id":"f711a3e3","metadata":{"id":"f711a3e3"},"source":["## Part E — Sensitivity Check (Top 1% Trim)"]},{"cell_type":"code","execution_count":null,"id":"31e0591b","metadata":{"id":"31e0591b"},"outputs":[],"source":["\n","cut = np.quantile(x, 0.99)\n","x_trim = x[x <= cut]\n","\n","ln_shape_t, ln_loc_t, ln_scale_t = st.lognorm.fit(x_trim, floc=0)\n","mu_t, sigma_t = float(np.log(ln_scale_t)), float(ln_shape_t)\n","\n","print({'Base_sigma': float(ln_shape), 'Trim_sigma': sigma_t})\n"]},{"cell_type":"markdown","id":"ea1941d1","metadata":{"id":"ea1941d1"},"source":["> **Comment on robustness to extremes and implications for pricing/reserving.**"]},{"cell_type":"markdown","source":["## Bonus (+5 pts)\n","Fit a truncated Lognormal or mixture (e.g., 2-component Lognormal) and compare AIC/BIC.\n","\n","##Grading Rubric (100 pts)\n","• EDA: 10\n","\n","• Fits: 30\n","\n","• Selection: 30\n","\n","• Tail risk: 20\n","\n","• Sensitivity: 10\n","\n","• Bonus: +5"],"metadata":{"id":"thPrHhEQnfpV"},"id":"thPrHhEQnfpV"}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}